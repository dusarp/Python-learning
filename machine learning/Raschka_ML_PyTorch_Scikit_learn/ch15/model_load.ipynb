{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your saved model file OK\n",
    "model_path = \"model.pt\"\n",
    "\n",
    "# Define the device (CPU or GPU) OK\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model state dictionary OK\n",
    "model_state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Create an empty model instance OK\n",
    "model = ... # Replace with your model architecture definition\n",
    "\n",
    "# Load the state dictionary into the model OK\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode (optional, but recommended)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"20240226_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240226_model.pt\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)\n",
    "    \n",
    "#vocab_size = len(char_array)\n",
    "vocab_size = 80 #ezt a harmadik filebol nyertem ki\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char2int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe island\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, starting_str, len_generated_text, scale_factor)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(model, starting_str, \n\u001b[1;32m      3\u001b[0m            len_generated_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, \n\u001b[1;32m      4\u001b[0m            scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([char2int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m starting_str])\n\u001b[1;32m      7\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(encoded_input, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m     generated_str \u001b[38;5;241m=\u001b[39m starting_str\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(model, starting_str, \n\u001b[1;32m      3\u001b[0m            len_generated_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, \n\u001b[1;32m      4\u001b[0m            scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mchar2int\u001b[49m[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m starting_str])\n\u001b[1;32m      7\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(encoded_input, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m     generated_str \u001b[38;5;241m=\u001b[39m starting_str\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char2int' is not defined"
     ]
    }
   ],
   "source": [
    "#helper function\n",
    "def sample(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           scale_factor=1.0):\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    hidden = hidden.to('cpu')\n",
    "    cell = cell.to('cpu')\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
    "    \n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model.to('cpu')\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Raschka-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
